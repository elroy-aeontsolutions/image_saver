{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scrape and Save\n",
        "This notebook was written to automate the scraping and saving of images from the internet."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the relevant modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-3.8.5-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.27.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (4.64.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.9.24)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.9)\n",
            "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm->webdriver-manager) (0.4.4)\n",
            "Installing collected packages: python-dotenv, webdriver-manager\n",
            "Successfully installed python-dotenv-0.21.0 webdriver-manager-3.8.5\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install webdriver-manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "TwbZjR86wQDD",
        "outputId": "2c432732-6f1a-4065-9dcb-17a2c54f2834"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "from selenium import webdriver \n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next step is to get the html as text from the url."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[WDM] - Downloading: 100%|██████████| 6.58M/6.58M [00:00<00:00, 8.33MB/s]\n"
          ]
        }
      ],
      "source": [
        "#chromePath='C:\\\\Users\\\\Lenovo\\Downloads\\\\chromedriver_win32'\n",
        "#driver=webdriver.Chrome(chromePath) \n",
        "\n",
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
        "\n",
        "url = \"https://www.thecarconnection.com/overview/acura_ilx_2022\"\n",
        "driver.get(url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we use BeautifulSoup to parse the html text data for the image links and then save them in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
        "\n",
        "html = driver.page_source\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "containers = soup.findAll('div', {'class':\"isv-r PNCib MSM1fd BUooTd\"} )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "os is used to a make the folder into which the files will be saved. A list of directory names corresponding to the category of images should be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Lenovo\\\\image_saver'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dirs = ['C:\\\\Users\\\\Lenovo\\\\Images']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, iterating through the list of image urls, download and save the images in the appropriate directory. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_elements = soup.find_all('img')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_urls = []\n",
        "for element in image_elements:\n",
        "     img_urls.append(element['src'])  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i, url in enumerate(img_urls):\n",
        "  try: response = requests.get(url)\n",
        "  except: pass\n",
        "  \n",
        "  if response.status_code:\n",
        "    with open('{0}/test_save_{1}.png'.format(dirs[0], i), 'wb') as fp:\n",
        "          fp.write(response.content)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Saving to Google Cloud\n",
        "\n",
        "1. Install gcloud https://cloud.google.com/sdk/docs/install#installation_instructions. \n",
        "\n",
        "2. Follow the instructions here https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev. \n",
        "\n",
        "3. Follow the instructions at https://cloud.google.com/docs/authentication/client-libraries to set up Google API credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "import glob\n",
        "\n",
        "project_id = 'vehicle-recognition-model'\n",
        "\n",
        "storage_client = storage.Client(project=project_id)\n",
        "\n",
        "bucket_name = 'vehicle-recognition-model'\n",
        "\n",
        "bucket = storage_client.get_bucket(bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "for str_path_file in glob.glob(dirs[0] + '/**'):\n",
        "    blob = bucket.blob(str_path_file)\n",
        "    blob.upload_from_filename(str_path_file)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll need to change this script in the following ways\n",
        "\n",
        "\n",
        "* It will need to be able to download text from an accessible website or google images; https://www.thecarconnection.com/ is not accessible via this method.\n",
        "* Depending on the website, the script may need to be able to perform searches \n",
        "* We will need to direct it to save the images in a Google Cloud bucket directory.\n",
        "* Filter out unneccsary images\n",
        "* How much is too much to download at a time\n",
        "* Upload directly to cloud bucket\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "20a9e06a1eee47c4abbed4ec8225ad91d78d9800d202b71b6b0a6e47016c6abd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
